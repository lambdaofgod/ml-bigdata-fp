{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for BMML Coursera course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 \n",
    "\n",
    "**Variational Inference in a nutshell**\n",
    "\n",
    "Assume your data follow hard distribution $p(v | \\theta)$ ($\\theta$ - model parameters).\n",
    "\n",
    "Assume there are hidden variables $h$ and a joint distribution $p(v, h| \\theta$) (this distribution doesn't have to be particularly nice).\n",
    "\n",
    "The variational inference proceeds by setting up distribution $q(h|v, \\theta)$\n",
    "\n",
    "Noting that we can compute KL divergence:\n",
    "\n",
    "$$KL(q(h|v \\theta), p(h|v, \\theta)) = $$\n",
    "$$\\mathbb{E}_{[q]} \\log q(h|v \\theta) - \\mathbb{E}_{[q]} \\log p(h|v, \\theta) = $$\n",
    "$$\\mathbb{E}_{[q]} \\log q(h|v \\theta) - \\mathbb{E}_{[q]} \\log (p(h, v| \\theta)p(v | \\theta)) = $$\n",
    "$$\\mathbb{E}_{[q]} \\log q(h|v \\theta) - \\mathbb{E}_{[q]} \\log p(h, v| \\theta) + \\mathbb{E}_{[q]} \\log p(v | \\theta)) $$\n",
    "\n",
    "After rearranging the terms we can see \n",
    "\n",
    "$$\\log p(v | \\theta))  = KL(q(h|v \\theta), p(h|v, \\theta)) -\\mathbb{E}_{[q]} \\log q(h|v \\theta) + \\mathbb{E}_{[q]} \\log p(h, v| \\theta)$$\n",
    "\n",
    "So, since KL-divergence is nonnegative,\n",
    "\n",
    "$$\\log p(v | \\theta)) \\geq \\mathbb{E}_{[q]} \\log p(h, v| \\theta) - \\mathbb{E}_{[q]} \\log q(h|v \\theta)$$\n",
    "\n",
    "The right hand term is called Evidence Lower Bound (ELBO). This means that variational methods can approximate Maximum Likelihood learning for $p(x|\\theta)$ maximizing this lower bound.\n",
    "\n",
    "### Tutorials on Variational Inference:\n",
    "\n",
    "Blei, [Variational Inference](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)\n",
    "\n",
    "[Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/091117.pdf), Chapters 11, 20 (also 28)\n",
    "\n",
    "### Topic models:\n",
    "\n",
    "Latent Dirichlet Allocation - BRML, ch. 20 section 6"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
